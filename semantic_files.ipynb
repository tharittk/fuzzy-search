{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bcc38bc",
   "metadata": {},
   "source": [
    "# The idea\n",
    "\n",
    "1) Traverse the file system from 'root'. \n",
    "2) For each directory or file, recurse\n",
    "3) For each file, split the full directory to dirtory name for each level, and append to the list. Use this list to construct the 'metadata' or 'tag' for the file (The directory name/path likely to capture the semantic meaning of the file).\n",
    "\n",
    "**Ignore embedded the full documents** \n",
    "\n",
    "4) For the file itself, with the filename + its tags from previous step, use LLM to write short 'descriptions' / keywords.\n",
    "5) Build the vector store in which indexed / embedded content is (description) and store meta data (entry_id)\n",
    "6) Store (entry_id, full_path) to the hash table\n",
    "\n",
    "The vector database is built/query with Ollama embedding (locally hosted). The description from keywork is written by OpenAI API for best result.\n",
    "\n",
    "User -> OpenAI's GPT-4o -> Parse To Keyword -> Vector DB -> Context (possible matches etc.) -> GPT-4o -> Deliver to users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1841c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "import chromadb\n",
    "import os\n",
    "\n",
    "model_endpoint = \"http://localhost:11435/\"\n",
    "embedding_model = \"nomic-embed-text\"\n",
    "\n",
    "ollama_embedding = OllamaEmbedding(\n",
    "    model_name=embedding_model,\n",
    "    base_url=model_endpoint,\n",
    ")\n",
    "class VectorIndex:\n",
    "    def __init__(self, persist_dir=\"chroma_store\") -> None:\n",
    "        self.persist_dir = persist_dir\n",
    "\n",
    "        # Initialize Chroma DB client with persistence\n",
    "        self.chroma_client = chromadb.PersistentClient(path=self.persist_dir)\n",
    "        self.chroma_collection = self.chroma_client.get_or_create_collection(name=\"docs\")\n",
    "\n",
    "        # Connect Chroma to LlamaIndex\n",
    "        self.vector_store = ChromaVectorStore(chroma_collection=self.chroma_collection)\n",
    "        self.storage_context = StorageContext.from_defaults(vector_store=self.vector_store)\n",
    "        \n",
    "    def build_vector_index_database(self, root_dir):\n",
    "        nodes = []\n",
    "        for subdir, _, files in os.walk(root_dir):\n",
    "            if \"/.\" in subdir: \n",
    "                continue\n",
    "            for f in files:\n",
    "                if f.endswith(\".DS_Store\") or f.endswith(\".bin\") or f.endswith(\"chroma.sqlite3\"):\n",
    "                    continue\n",
    "                \n",
    "                file_path = os.path.join(subdir, f)\n",
    "                tracks = file_path[len(root_dir):].split(\"/\") # root_dir info is implicitly shared\n",
    "               \n",
    "                node = TextNode(text=str(tracks)) \n",
    "                nodes.append(node)\n",
    "        \n",
    "        self.storage_context.persist()\n",
    "        return VectorStoreIndex(nodes, embed_model=ollama_embedding)\n",
    "   \n",
    "indexer = VectorIndex()\n",
    "ROOT_DIR ='/home/sdaadmin/tharitt_working/' \n",
    "index = indexer.build_vector_index_database(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41c68b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "Node ID: 05b38408-8481-44b7-a859-f6f663ca6d56\n",
      "Text: ['qi-llama', 'qi', 'qi_tools.py']\n",
      "Score:  0.731\n",
      "\n",
      "Result 2:\n",
      "Node ID: ab33831e-6ff7-4cf7-8f92-a660153ce74f\n",
      "Text: ['qi-llama', 'qi', '__pycache__', 'qi_tools.cpython-313.pyc']\n",
      "Score:  0.716\n",
      "\n",
      "Result 3:\n",
      "Node ID: be91b241-fab6-4a6a-8499-b4d39aaf4891\n",
      "Text: ['qi-llama', 'qi', '__pycache__', 'qi_tools.cpython-39.pyc']\n",
      "Score:  0.712\n",
      "\n",
      "Result 4:\n",
      "Node ID: a1d17967-3f17-4afb-b5bf-c7ffea3a4ded\n",
      "Text: ['qi-llama', 'qi', 'qi_bongkot_loader.py']\n",
      "Score:  0.708\n",
      "\n",
      "Result 5:\n",
      "Node ID: 8376a85b-0fd9-4c13-adc4-31f625ad634b\n",
      "Text: ['qi-llama', 'qi', 'qi_well.py']\n",
      "Score:  0.707\n",
      "\n",
      "Result 6:\n",
      "Node ID: b1334546-3fe6-4158-970b-2d73a5d77841\n",
      "Text: ['qi-llama', 'qi', '__pycache__',\n",
      "'qi_bongkot_loader.cpython-313.pyc']\n",
      "Score:  0.705\n",
      "\n",
      "Result 7:\n",
      "Node ID: f2f54caf-759c-44aa-9c80-b670bce01de9\n",
      "Text: ['qi-llama', 'qi', 'qi_lang.py']\n",
      "Score:  0.701\n",
      "\n",
      "Result 8:\n",
      "Node ID: 90ed4d12-9afb-41c3-8d28-943f9588fd93\n",
      "Text: ['qi-llama', 'qi', 'qi_arthit_loader.py']\n",
      "Score:  0.697\n",
      "\n",
      "Result 9:\n",
      "Node ID: 4eb9582a-c8c2-4803-a916-65d0c833c776\n",
      "Text: ['qi-llama', 'qi', '__pycache__',\n",
      "'qi_arthit_loader.cpython-313.pyc']\n",
      "Score:  0.696\n",
      "\n",
      "Result 10:\n",
      "Node ID: 49e83fe5-4fd3-44ca-839c-20afbfd8639b\n",
      "Text: ['qi-llama', 'qi', '__pycache__', 'qi_well.cpython-313.pyc']\n",
      "Score:  0.694\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retriever = index.as_retriever(similarity_top_k=10, sparse_top_k=None)\n",
    "query = \"look for the file that describe python qi functions in rock physics catalog\"\n",
    "nodes = retriever.retrieve(query)\n",
    "\n",
    "# Print the top-k matching documents\n",
    "for i, node in enumerate(nodes):\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71d2ef38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on your query regarding the file that describes Python functions related to the \"qi\" (rock physics catalog), I have identified a few potential file paths from the provided list. Here are the most relevant ones:\n",
      "\n",
      "1. `/home/sdaadmin/tharitt_working/qi-llama/qi/qi_tools.py`\n",
      "2. `/home/sdaadmin/tharitt_working/qi-llama/qi/qi_bongkot_loader.py`\n",
      "3. `/home/sdaadmin/tharitt_working/qi-llama/qi/qi_well.py`\n",
      "4. `/home/sdaadmin/tharitt_working/qi-llama/qi/qi_lang.py`\n",
      "5. `/home/sdaadmin/tharitt_working/qi-llama/qi/qi_arthit_loader.py`\n",
      "\n",
      "These files seem to be related to the \"qi\" functions in the rock physics catalog. If you need more specific details or further assistance, please let me know!\n"
     ]
    }
   ],
   "source": [
    "from openai_service import OpenAIService\n",
    "llm = OpenAIService()\n",
    "system_prompt=f\"The user is looking for the full file path of the intended file. Given the context which describe the path in the list format\\\n",
    "    {nodes}. This comes from semantic search. The prefix is {ROOT_DIR}. The user query is {query}. \\\n",
    "        If there are multiple path that may match, pick few that you feel confident\"\n",
    "data = llm.create_data(\n",
    "        system=system_prompt,\n",
    "        prompt=\"Give a well structured, human-like response to the user.\"\n",
    "        )\n",
    "llm = OpenAIService()\n",
    "response = llm.create_request(data)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
